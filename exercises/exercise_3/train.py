import streamlit as st
import numpy as np
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import silhouette_score
import mlflow
import os
import dagshub
import datetime
from tensorflow.keras.datasets import mnist
import openml

# H√†m kh·ªüi t·∫°o MLflow
def mlflow_input():
    DAGSHUB_MLFLOW_URI = "https://dagshub.com/VietNam0410/ML_v3.mlflow"
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    
    os.environ["MLFLOW_TRACKING_USERNAME"] = "VietNam0410"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "c9db6bdcca1dfed76d2af2cdb15a9277e6732d6b"
    
    try:
        client = mlflow.tracking.MlflowClient()
        experiments = client.search_experiments()
        if experiments:
            st.success("K·∫øt n·ªëi MLflow v·ªõi DagsHub th√†nh c√¥ng! ‚úÖ")
        else:
            st.warning("Kh√¥ng t√¨m th·∫•y experiment n√†o, nh∆∞ng k·∫øt n·ªëi MLflow v·∫´n ho·∫°t ƒë·ªông.")
        return "ML_v3"
    except mlflow.exceptions.MlflowException as e:
        st.error(f"L·ªói x√°c th·ª±c MLflow: {str(e)}. Vui l√≤ng ki·ªÉm tra token t·∫°i https://dagshub.com/user/settings/tokens.")
        return None

# H√†m t·∫£i d·ªØ li·ªáu MNIST
@st.cache_data
def load_mnist():
    with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu MNIST..."):
        try:
            dataset = openml.datasets.get_dataset(554)
            X, y, _, _ = dataset.get_data(target='class')
            X = X.values.reshape(-1, 28 * 28) / 255.0
            y = y.astype(np.int32)
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ t·∫£i t·ª´ OpenML: {str(e)}. S·ª≠ d·ª•ng TensorFlow.")
            (X_train, y_train), (X_test, y_test) = mnist.load_data()
            X = np.concatenate([X_train, X_test], axis=0) / 255.0
            y = np.concatenate([y_train, y_test], axis=0)
            X = X.reshape(-1, 28 * 28)
        return X, y

# Cache scaler v√† m√¥ h√¨nh ƒë·ªÉ t√°i s·ª≠ d·ª•ng
@st.cache_resource
def get_scaler():
    return StandardScaler()

@st.cache_resource
def get_model(model_choice, **params):
    if model_choice == "K-means":
        return KMeans(**params)
    else:
        return DBSCAN(**params)

def train_clustering():
    st.header("Hu·∫•n luy·ªán M√¥ h√¨nh Clustering tr√™n MNIST üßÆ")

    # ƒê√≥ng run MLflow n·∫øu ƒëang ho·∫°t ƒë·ªông
    if mlflow.active_run():
        mlflow.end_run()
        st.info("ƒê√£ ƒë√≥ng run MLflow tr∆∞·ªõc ƒë√≥.")

    # Thi·∫øt l·∫≠p MLflow
    DAGSHUB_REPO = mlflow_input()
    if DAGSHUB_REPO is None:
        st.error("Kh√¥ng th·ªÉ ti·∫øp t·ª•c do l·ªói k·∫øt n·ªëi MLflow.")
        return

    experiment_name = "MNIST_Train_Clustering"
    with st.spinner("ƒêang thi·∫øt l·∫≠p Experiment..."):
        try:
            client = mlflow.tracking.MlflowClient()
            experiment = client.get_experiment_by_name(experiment_name)
            if experiment and experiment.lifecycle_stage == "deleted":
                st.warning(f"Experiment '{experiment_name}' ƒë√£ b·ªã x√≥a. Vui l√≤ng kh√¥i ph·ª•c qua DagsHub UI.")
                return
            else:
                mlflow.set_experiment(experiment_name)
        except mlflow.exceptions.MlflowException as e:
            st.error(f"L·ªói khi thi·∫øt l·∫≠p experiment: {str(e)}.")
            return

    # T·∫£i d·ªØ li·ªáu
    X_full, y_full = load_mnist()
    total_samples = len(X_full)

    st.subheader("Chia t·∫≠p d·ªØ li·ªáu MNIST üîÄ")
    max_samples = st.slider("S·ªë m·∫´u t·ªëi ƒëa (0 = to√†n b·ªô, t·ªëi ƒëa 70.000)", 0, 70000, 70000, step=100)  # Gi·ªØ nguy√™n m·∫∑c ƒë·ªãnh 70,000
    
    if max_samples == 0 or max_samples > total_samples:
        st.warning(f"S·ªë m·∫´u {max_samples} v∆∞·ª£t qu√° {total_samples}. D√πng to√†n b·ªô.")
        max_samples = total_samples
    elif max_samples < total_samples:
        indices = np.random.choice(total_samples, max_samples, replace=False)
        X_full = X_full[indices]
        y_full = y_full[indices]

    test_size = st.slider("T·ª∑ l·ªá t·∫≠p ki·ªÉm tra (%)", 10, 50, 20, step=5) / 100
    train_size_relative = st.slider("T·ª∑ l·ªá t·∫≠p hu·∫•n luy·ªán (%)", 50, 90, 70, step=5) / 100
    val_size_relative = 1 - train_size_relative

    X_remaining, X_test, y_remaining, y_test = train_test_split(
        X_full, y_full, test_size=test_size, random_state=42, stratify=y_full
    )
    X_train, X_valid, y_train, y_valid = train_test_split(
        X_remaining, y_remaining, train_size=train_size_relative, random_state=42, stratify=y_remaining
    )

    st.subheader("Th√¥ng tin t·∫≠p d·ªØ li·ªáu üìä")
    st.write(f"T·ªïng s·ªë m·∫´u: {max_samples}")
    st.write(f"T·ª∑ l·ªá: Hu·∫•n luy·ªán {train_size_relative*100:.1f}%, Validation {val_size_relative*100:.1f}%, Ki·ªÉm tra {test_size*100:.1f}%")
    st.write(f"T·∫≠p hu·∫•n luy·ªán: {len(X_train)} m·∫´u, Validation: {len(X_valid)} m·∫´u, Ki·ªÉm tra: {len(X_test)} m·∫´u")

    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler = get_scaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_valid_scaled = scaler.transform(X_valid)
    X_test_scaled = scaler.transform(X_test)

    st.subheader("Gi·ªõi thi·ªáu thu·∫≠t to√°n Clustering")
    st.write("### K-means")
    st.write("Ph√¢n c·ª•m d·ªØ li·ªáu th√†nh K c·ª•m d·ª±a tr√™n kho·∫£ng c√°ch t·ªõi t√¢m c·ª•m.")
    st.write("### DBSCAN")
    st.write("Ph√¢n c·ª•m d·ª±a tr√™n m·∫≠t ƒë·ªô, t·ª± ƒë·ªông ph√°t hi·ªán nhi·ªÖu.")

    st.subheader("Hu·∫•n luy·ªán M√¥ h√¨nh üéØ")
    model_choice = st.selectbox("Ch·ªçn thu·∫≠t to√°n", ["K-means", "DBSCAN"])

    if model_choice == "K-means":
        n_clusters = st.slider("S·ªë c·ª•m (K)", 2, 20, 10, step=1)
        model_params = {"n_clusters": n_clusters, "random_state": 42}  # Kh√¥ng t·ªëi ∆∞u n_init, max_iter
    else:
        eps = st.slider("Kho·∫£ng c√°ch t·ªëi ƒëa (eps)", 0.1, 2.0, 0.5, step=0.1)
        min_samples = st.slider("S·ªë ƒëi·ªÉm t·ªëi thi·ªÉu", 2, 20, 5, step=1)
        model_params = {"eps": eps, "min_samples": min_samples}  # Kh√¥ng d√πng n_jobs

    run_name = st.text_input("Nh·∫≠p t√™n Run ID (ƒë·ªÉ tr·ªëng ƒë·ªÉ t·ª± t·∫°o)", value="", max_chars=20)
    if not run_name.strip():
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        run_name = f"MNIST_{model_choice}_{timestamp}"

    if st.button("Hu·∫•n luy·ªán v√† ƒë√°nh gi√°"):
        if mlflow.active_run():
            mlflow.end_run()

        with st.spinner(f"ƒêang hu·∫•n luy·ªán {model_choice}..."):
            model = get_model(model_choice, **model_params)
            model.fit(X_train_scaled)

            # D·ª± ƒëo√°n nh√£n
            labels_train = model.predict(X_train_scaled) if model_choice == "K-means" else model.fit_predict(X_train_scaled)
            labels_valid = model.predict(X_valid_scaled) if model_choice == "K-means" else model.fit_predict(X_valid_scaled)
            labels_test = model.predict(X_test_scaled) if model_choice == "K-means" else model.fit_predict(X_test_scaled)
            n_clusters_found = len(np.unique(labels_valid)) - (1 if -1 in labels_valid else 0)

            # T√≠nh Silhouette Score
            silhouette_train = silhouette_score(X_train_scaled, labels_train) if n_clusters_found > 1 else -1
            silhouette_valid = silhouette_score(X_valid_scaled, labels_valid) if n_clusters_found > 1 else -1

            # Hi·ªÉn th·ªã th√¥ng tin
            st.write(f"**Thu·∫≠t to√°n**: {model_choice}")
            st.write(f"**Tham s·ªë**: {model_params}")
            st.write("**Ch·ªâ s·ªë ƒë√°nh gi√°**:")
            st.write(f"- Silhouette Score (Train): {silhouette_train:.4f}")
            st.write(f"- Silhouette Score (Valid): {silhouette_valid:.4f}")
            st.write("""
                **Th√¥ng tin v·ªÅ Silhouette Score**:  
                - L√† ch·ªâ s·ªë ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng ph√¢n c·ª•m, ƒëo l∆∞·ªùng m·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa m·ªôt ƒëi·ªÉm trong c·ª•m c·ªßa n√≥ so v·ªõi c√°c c·ª•m kh√°c.  
                - Gi√° tr·ªã t·ª´ -1 ƒë·∫øn 1:  
                  + G·∫ßn 1: C√°c c·ª•m ƒë∆∞·ª£c ph√¢n t√°ch t·ªët, ƒëi·ªÉm n·∫±m g·∫ßn c·ª•m c·ªßa n√≥.  
                  + G·∫ßn 0: C√°c c·ª•m ch·ªìng l·∫•p nhau.  
                  + G·∫ßn -1: ƒêi·ªÉm c√≥ th·ªÉ b·ªã ph√¢n c·ª•m sai.  
                - Ch·ªâ t√≠nh khi s·ªë c·ª•m > 1.
            """)

            # Logging v√†o MLflow
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with mlflow.start_run(run_name=run_name) as run:
                mlflow.log_param("timestamp", timestamp)
                mlflow.log_param("run_id", run.info.run_id)
                mlflow.log_params(model_params)
                mlflow.log_param("model_type", model_choice)
                mlflow.log_param("max_samples", max_samples)
                mlflow.log_param("test_size", test_size)
                mlflow.log_param("train_size", train_size_relative)
                mlflow.log_param("val_size", val_size_relative)
                mlflow.log_metric("n_clusters_found", n_clusters_found)
                mlflow.log_metric("silhouette_train", silhouette_train)
                mlflow.log_metric("silhouette_valid", silhouette_valid)
                mlflow.sklearn.log_model(model, "model", input_example=X_train_scaled[:1])
                mlflow.sklearn.log_model(scaler, "scaler", input_example=X_train[:1])

                run_id = run.info.run_id
                mlflow_uri = st.session_state['mlflow_url']
                st.success(f"Hu·∫•n luy·ªán {model_choice} ho√†n t·∫•t! ‚úÖ (Run: {run_name}, ID: {run_id}, Th·ªùi gian: {timestamp})")
                st.markdown(f"Xem chi ti·∫øt t·∫°i: [DagsHub MLflow]({mlflow_uri})")

            # L∆∞u v√†o session_state
            st.session_state['clustering_model'] = model
            st.session_state['clustering_scaler'] = scaler
            st.session_state['clustering_labels_train'] = labels_train
            st.session_state['clustering_labels_valid'] = labels_valid
            st.session_state['clustering_labels_test'] = labels_test

if __name__ == "__main__":
    train_clustering()