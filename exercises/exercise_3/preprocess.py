import streamlit as st
import numpy as np
import openml
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import mnist
import mlflow
import os
import dagshub
import datetime

# H√†m kh·ªüi t·∫°o MLflow
def mlflow_input():
    DAGSHUB_MLFLOW_URI = "https://dagshub.com/VietNam0410/vn0410.mlflow"
    mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
    st.session_state['mlflow_url'] = DAGSHUB_MLFLOW_URI
    os.environ["MLFLOW_TRACKING_USERNAME"] = "VietNam0410"
    os.environ["MLFLOW_TRACKING_PASSWORD"] = "22fd02345f8ff45482a20960058627630acaf190"  # Thay b·∫±ng token c√° nh√¢n c·ªßa b·∫°n
    DAGSHUB_REPO = "vn0410"
    return DAGSHUB_REPO

# Cache d·ªØ li·ªáu MNIST
@st.cache_data
def load_mnist_from_openml():
    with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu MNIST..."):
        try:
            dataset = openml.datasets.get_dataset(554)
            X, y, _, _ = dataset.get_data(target='class')
            X = X.values.reshape(-1, 28, 28, 1) / 255.0
            y = y.astype(np.int32)
            return X, y
        except Exception as e:
            st.error(f"Kh√¥ng th·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ OpenML. S·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ TensorFlow: {str(e)}")
            (X_train, y_train), (X_test, y_test) = mnist.load_data()
            X = np.concatenate([X_train, X_test], axis=0) / 255.0
            y = np.concatenate([y_train, y_test], axis=0)
            return X.reshape(-1, 28, 28, 1), y

def preprocess_mnist_clustering():
    st.header("Ti·ªÅn x·ª≠ l√Ω D·ªØ li·ªáu MNIST cho Clustering üñåÔ∏è")

    # ƒê√≥ng b·∫•t k·ª≥ run n√†o ƒëang ho·∫°t ƒë·ªông ƒë·ªÉ tr√°nh xung ƒë·ªôt khi b·∫Øt ƒë·∫ßu
    if mlflow.active_run():
        mlflow.end_run()
        st.info("ƒê√£ ƒë√≥ng run MLflow ƒëang ho·∫°t ƒë·ªông tr∆∞·ªõc ƒë√≥.")

    # G·ªçi h√†m mlflow_input ƒë·ªÉ thi·∫øt l·∫≠p MLflow t·∫°i DAGSHUB_MLFLOW_URI
    DAGSHUB_REPO = mlflow_input()

    # Cho ng∆∞·ªùi d√πng ƒë·∫∑t t√™n Experiment
    experiment_name = st.text_input("Nh·∫≠p t√™n Experiment cho ti·ªÅn x·ª≠ l√Ω", value="MNIST_Clustering_Preprocessing")
    with st.spinner("ƒêang thi·∫øt l·∫≠p Experiment tr√™n DagsHub MLflow..."):
        try:
            client = mlflow.tracking.MlflowClient()
            experiment = client.get_experiment_by_name(experiment_name)
            if experiment and experiment.lifecycle_stage == "deleted":
                st.warning(f"Experiment '{experiment_name}' ƒë√£ b·ªã x√≥a tr∆∞·ªõc ƒë√≥. Vui l√≤ng ch·ªçn t√™n kh√°c ho·∫∑c kh√¥i ph·ª•c experiment qua DagsHub UI.")
                new_experiment_name = st.text_input("Nh·∫≠p t√™n Experiment m·ªõi", value=f"{experiment_name}_Restored_{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                if new_experiment_name:
                    mlflow.set_experiment(new_experiment_name)
                    experiment_name = new_experiment_name
                else:
                    st.error("Vui l√≤ng nh·∫≠p t√™n experiment m·ªõi ƒë·ªÉ ti·∫øp t·ª•c.")
                    return
            else:
                mlflow.set_experiment(experiment_name)
        except Exception as e:
            st.error(f"L·ªói khi thi·∫øt l·∫≠p experiment: {str(e)}")
            return

    # T·∫£i d·ªØ li·ªáu MNIST
    if 'X_full_clustering' not in st.session_state or 'y_full_clustering' not in st.session_state:
        st.session_state['X_full_clustering'], st.session_state['y_full_clustering'] = load_mnist_from_openml()
        st.success("D·ªØ li·ªáu MNIST ƒë√£ ƒë∆∞·ª£c t·∫£i v√† chu·∫©n h√≥a th√†nh c√¥ng! ‚úÖ")

    X_full = st.session_state['X_full_clustering']
    y_full = st.session_state['y_full_clustering']
    total_samples = len(X_full)

    st.subheader("Th√¥ng tin D·ªØ li·ªáu MNIST ƒê·∫ßy ƒë·ªß üîç")
    st.write(f"T·ªïng s·ªë l∆∞·ª£ng m·∫´u: {total_samples}")

    st.subheader("Chia t√°ch D·ªØ li·ªáu (T√πy ch·ªçn) üîÄ")
    max_samples = st.slider("Ch·ªçn s·ªë l∆∞·ª£ng m·∫´u t·ªëi ƒëa (0 ƒë·ªÉ d√πng to√†n b·ªô)", 0, total_samples, total_samples, step=100)
    
    if max_samples == 0:
        max_samples = total_samples
    elif max_samples > total_samples:
        st.error(f"S·ªë l∆∞·ª£ng m·∫´u ({max_samples}) v∆∞·ª£t qu√° t·ªïng s·ªë m·∫´u c√≥ s·∫µn ({total_samples}). ƒê·∫∑t l·∫°i v·ªÅ {total_samples}.")
        max_samples = total_samples

    test_size = st.slider("Ch·ªçn t·ª∑ l·ªá t·∫≠p ki·ªÉm tra (%)", min_value=10, max_value=50, value=20, step=5) / 100
    remaining_size = 1 - test_size
    train_size_relative = st.slider(
        "Ch·ªçn t·ª∑ l·ªá t·∫≠p hu·∫•n luy·ªán (% tr√™n ph·∫ßn c√≤n l·∫°i sau khi tr·ª´ t·∫≠p test)",
        min_value=10, max_value=90, value=70, step=5
    ) / 100

    # T√≠nh to√°n t·ª∑ l·ªá t·∫≠p train v√† validation d·ª±a tr√™n ph·∫ßn c√≤n l·∫°i (remaining_size)
    train_size = remaining_size * train_size_relative
    val_size = remaining_size * (1 - train_size_relative)

    # Hi·ªÉn th·ªã t·ª∑ l·ªá th·ª±c t·∫ø d·ª±a tr√™n to√†n b·ªô d·ªØ li·ªáu
    st.write(f"T·ª∑ l·ªá th·ª±c t·∫ø: Hu·∫•n luy·ªán {train_size*100:.1f}%, Validation {val_size*100:.1f}%, Ki·ªÉm tra {test_size*100:.1f}%")
    st.write(f"Ki·ªÉm tra t·ªïng t·ª∑ l·ªá: {train_size*100 + val_size*100 + test_size*100:.1f}% (ph·∫£i lu√¥n b·∫±ng 100%)")

    # Cho ph√©p ng∆∞·ªùi d√πng ƒë·∫∑t t√™n run ID cho vi·ªác chia d·ªØ li·ªáu
    run_name = st.text_input("Nh·∫≠p t√™n Run ID cho vi·ªác chia d·ªØ li·ªáu (ƒë·ªÉ tr·ªëng ƒë·ªÉ t·ª± ƒë·ªông t·∫°o)", value="", max_chars=20, key="data_split_run_name_input")
    if run_name.strip() == "":
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        run_name = f"MNIST_Clustering_DataSplit_{timestamp.replace(' ', '_').replace(':', '-')}"  # ƒê·ªãnh d·∫°ng t√™n run h·ª£p l·ªá cho MLflow

    if st.button("Chia d·ªØ li·ªáu"):
        with st.spinner("ƒêang chia d·ªØ li·ªáu..."):
            if max_samples < total_samples:
                indices = np.random.choice(total_samples, max_samples, replace=False)
                X_subset = X_full[indices]
                y_subset = y_full[indices]
            else:
                X_subset = X_full
                y_subset = y_full

            # Chia t·∫≠p test tr∆∞·ªõc
            X_remaining, X_test, y_remaining, y_test = train_test_split(
                X_subset, y_subset, test_size=test_size, random_state=42
            )
            # Chia t·∫≠p train v√† validation t·ª´ ph·∫ßn c√≤n l·∫°i (remaining_size)
            X_train, X_valid, y_train, y_valid = train_test_split(
                X_remaining, y_remaining, train_size=train_size_relative, random_state=42
            )

            st.success(f"ƒê√£ chia d·ªØ li·ªáu v·ªõi s·ªë l∆∞·ª£ng m·∫´u: {max_samples}. K√≠ch th∆∞·ªõc: Hu·∫•n luy·ªán {train_size*100:.1f}%, Validation {val_size*100:.1f}%, Ki·ªÉm tra {test_size*100:.1f}%! ‚úÖ")
            st.write(f"T·∫≠p hu·∫•n luy·ªán: {len(X_train)} m·∫´u")
            st.write(f"T·∫≠p validation: {len(X_valid)} m·∫´u")
            st.write(f"T·∫≠p ki·ªÉm tra: {len(X_test)} m·∫´u")

            # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i tr∆∞·ªõc khi l∆∞u file
            processed_dir = "exercises/exercise_mnist/data/processed"
            os.makedirs(processed_dir, exist_ok=True)
            processed_file = os.path.join(processed_dir, "mnist_clustering_processed.npz")

            # L∆∞u d·ªØ li·ªáu c·ª•c b·ªô
            with st.spinner("ƒêang l∆∞u d·ªØ li·ªáu ƒë√£ chia..."):
                np.savez(processed_file, 
                         X_train=X_train, y_train=y_train,
                         X_valid=X_valid, y_valid=y_valid,
                         X_test=X_test, y_test=y_test)
                st.success(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o {processed_file} üíæ")

            # Logging v√†o MLflow t·∫°i DAGSHUB_MLFLOW_URI
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            with mlflow.start_run(run_name=run_name) as run:
                mlflow.log_param("timestamp", timestamp)
                mlflow.log_param("run_id", run.info.run_id)
                mlflow.log_param("max_samples", max_samples)
                mlflow.log_param("train_size", train_size)
                mlflow.log_param("val_size", val_size)
                mlflow.log_param("test_size", test_size)
                mlflow.log_metric("train_samples", len(X_train))
                mlflow.log_metric("valid_samples", len(X_valid))
                mlflow.log_metric("test_samples", len(X_test))

                # Log file d·ªØ li·ªáu ƒë√£ chia l√†m artifact
                mlflow.log_artifact(processed_file, artifact_path="processed_data")
                os.remove(processed_file)  # X√≥a file c·ª•c b·ªô sau khi log

                run_id = run.info.run_id
                mlflow_uri = st.session_state['mlflow_url']
                st.success(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia, l∆∞u c·ª•c b·ªô, v√† log v√†o DagsHub MLflow th√†nh c√¥ng! ‚úÖ (T√™n Run: {run_name}, Run ID: {run_id}, Th·ªùi gian: {timestamp})")
                st.markdown(f"Xem chi ti·∫øt t·∫°i: [DagsHub MLflow Tracking]({mlflow_uri})")

            # L∆∞u d·ªØ li·ªáu v√†o session_state ƒë·ªÉ s·ª≠ d·ª•ng sau
            st.session_state['mnist_clustering_data'] = {
                'X_train': X_train,
                'y_train': y_train,
                'X_valid': X_valid,
                'y_valid': y_valid,
                'X_test': X_test,
                'y_test': y_test
            }

if __name__ == "__main__":
    preprocess_mnist_clustering()