import os
import streamlit as st
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
import mlflow
import mlflow.keras
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from datetime import datetime

# Thi·∫øt l·∫≠p MLflow
DAGSHUB_MLFLOW_URI = "https://dagshub.com/VietNam0410/ML_v3.mlflow"
mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
os.environ["MLFLOW_TRACKING_USERNAME"] = "VietNam0410"
os.environ["MLFLOW_TRACKING_PASSWORD"] = "c9db6bdcca1dfed76d2af2cdb15a9277e6732d6b"

def plot_network_structure(n_hidden_layers, neurons_per_layer):
    """V·∫Ω bi·ªÉu ƒë·ªì 2D m√¥ t·∫£ c·∫•u tr√∫c m·∫°ng"""
    layers = ['Input (784)'] + [f'Hidden {i+1} ({neurons_per_layer})' for i in range(n_hidden_layers)] + ['Output (10)']
    x = list(range(len(layers)))
    y = [784] + [neurons_per_layer] * n_hidden_layers + [10]
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=x, y=y, mode='lines+markers', name='S·ªë n∆°-ron',
                             line=dict(color='blue'), marker=dict(size=10)))
    fig.update_layout(
        title="C·∫•u Tr√∫c Neural Network",
        xaxis_title="L·ªõp",
        yaxis_title="S·ªë N∆°-ron",
        xaxis=dict(tickmode='array', tickvals=x, ticktext=layers),
        height=400
    )
    return fig

def train_mnist(X_full, y_full):
    st.title("üß† Hu·∫•n Luy·ªán Neural Network")


    # Kh·ªüi t·∫°o session_state ƒë·ªÉ l∆∞u gi√° tr·ªã learning_rate
    if 'learning_rate' not in st.session_state:
        st.session_state['learning_rate'] = 0.001  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh

    # Ch·ªçn d·ªØ li·ªáu
    st.subheader("1. Ch·ªçn D·ªØ Li·ªáu")
    total_samples = X_full.shape[0]
    max_samples = st.slider("S·ªë m·∫´u hu·∫•n luy·ªán", 1000, total_samples, 10000, step=1000)
    if max_samples < total_samples:
        indices = np.random.choice(total_samples, max_samples, replace=False)
        X, y = X_full[indices], y_full[indices]
    else:
        X, y = X_full, y_full
    X = X.reshape(-1, 28, 28).astype('float32') / 255.0  # Chu·∫©n h√≥a
    y_cat = to_categorical(y, 10)
    st.write(f"ƒê√£ ch·ªçn {max_samples} m·∫´u ƒë·ªÉ hu·∫•n luy·ªán.")

    # Chia d·ªØ li·ªáu th√†nh train v√† test
    st.subheader("2. Chia D·ªØ Li·ªáu")
    test_size = st.slider("T·ª∑ l·ªá t·∫≠p test (%)", 10, 30, 20, step=5) / 100
    train_size = 1 - test_size

    X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=test_size, random_state=42, stratify=y)

    st.write(f"T·ª∑ l·ªá: Train {train_size*100:.1f}%, Test {test_size*100:.1f}%")
    st.write(f"S·ªë m·∫´u: Train {len(X_train)}, Test {len(X_test)}")

    # Tham s·ªë hu·∫•n luy·ªán
    st.subheader("3. Thi·∫øt L·∫≠p Tham S·ªë")
    st.markdown("T√πy ch·ªânh m·∫°ng c·ªßa b·∫°n (c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ train nhanh):")
    col1, col2 = st.columns(2)
    with col1:
        n_hidden_layers = st.slider("S·ªë l·ªõp ·∫©n", 1, 3, 2, help="S·ªë l·ªõp x·ª≠ l√Ω d·ªØ li·ªáu (2 l√† t·ªëi ∆∞u).")
        neurons_per_layer = st.selectbox("S·ªë n∆°-ron m·ªói l·ªõp", [64, 128, 256], index=1, help="128 n∆°-ron l√† c√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô v√† hi·ªáu su·∫•t.")
        epochs = st.slider("S·ªë v√≤ng l·∫∑p (epochs)", 5, 20, 5, help="5 epochs l√† ƒë·ªß ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c t·ªët.")
    with col2:
        batch_size = st.selectbox("K√≠ch th∆∞·ªõc batch", [32, 64, 128], index=1, help="64 l√† t·ªëi ∆∞u cho t·ªëc ƒë·ªô.")
        learning_rate = st.number_input(
            "T·ªëc ƒë·ªô h·ªçc (Œ∑)", 
            min_value=0.00001,
            max_value=0.1,
            value=float(st.session_state['learning_rate']),
            step=0.00001,
            key="learning_rate_input",
            help="Nh·∫≠p t·ªëc ƒë·ªô h·ªçc (g·ª£i √Ω: 0.001 l√† ph√π h·ª£p nh·∫•t cho MNIST, th·ª≠ t·ª´ 0.0001 ƒë·∫øn 0.01 v·ªõi ƒë·ªô ch√≠nh x√°c cao)."
        )
        activation = st.selectbox("H√†m k√≠ch ho·∫°t", ['relu', 'sigmoid', 'tanh', 'softmax'], index=0,
                                  help="ReLU th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t.")

    # C·∫≠p nh·∫≠t session_state v·ªõi gi√° tr·ªã learning_rate m·ªõi
    if learning_rate != st.session_state['learning_rate']:
        st.session_state['learning_rate'] = learning_rate
        st.write(f"ƒê√£ c·∫≠p nh·∫≠t t·ªëc ƒë·ªô h·ªçc: **{learning_rate}**")

    # Hu·∫•n luy·ªán
    if st.button("B·∫Øt ƒê·∫ßu Hu·∫•n Luy·ªán"):
        # X√¢y d·ª±ng m√¥ h√¨nh
        model = Sequential()
        model.add(Flatten(input_shape=(28, 28)))
        for _ in range(n_hidden_layers):
            model.add(Dense(neurons_per_layer, activation=activation if activation != 'softmax' else 'relu'))
            model.add(Dropout(0.2))  # Dropout c·ªë ƒë·ªãnh
        model.add(Dense(10, activation='softmax'))

        # Bi√™n d·ªãch m√¥ h√¨nh
        optimizer = Adam(learning_rate=st.session_state['learning_rate'])
        model.compile(optimizer=optimizer,
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])

        # MLflow
        mlflow.set_experiment("MNIST_Neural_Network")
        with mlflow.start_run(run_name=f"Train_{n_hidden_layers}_{neurons_per_layer}_{epochs}"):
            # Hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh 100% ngay l·∫≠p t·ª©c
            progress_bar = st.progress(1.0)
            st.write("Hu·∫•n luy·ªán ho√†n t·∫•t (thanh ti·∫øn tr√¨nh gi·∫£ l·∫≠p 100%).")

            # Hu·∫•n luy·ªán m√¥ h√¨nh
            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)

            # ƒê√°nh gi√° tr√™n t·∫≠p train v√† test
            train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
            test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.subheader("4. K·∫øt Qu·∫£ Hu·∫•n Luy·ªán")
            st.success(f"**ƒê·ªô ch√≠nh x√°c (Train)**: {train_acc:.4f}")
            st.success(f"**ƒê·ªô ch√≠nh x√°c (Test)**: {test_acc:.4f}")
            st.success(f"**M·∫•t m√°t (Train)**: {train_loss:.4f}")
            st.success(f"**M·∫•t m√°t (Test)**: {test_loss:.4f}")

            # Bi·ªÉu ƒë·ªì so s√°nh ƒë·ªô ch√≠nh x√°c v√† m·∫•t m√°t gi·ªØa train v√† test
            st.subheader("5. Hi·ªáu Su·∫•t So S√°nh Train v√† Test")
            fig = go.Figure()
            fig.add_trace(go.Bar(x=['Train', 'Test'], y=[train_acc, test_acc], name='Accuracy', marker_color=['blue', 'orange']))
            fig.add_trace(go.Bar(x=['Train', 'Test'], y=[train_loss, test_loss], name='Loss', marker_color=['green', 'red']))
            fig.update_layout(
                title="So s√°nh ƒê·ªô ch√≠nh x√°c v√† M·∫•t m√°t (Train vs Test)",
                xaxis_title="T·∫≠p d·ªØ li·ªáu",
                yaxis_title="Gi√° tr·ªã",
                barmode='group',
                height=400
            )
            st.plotly_chart(fig, use_container_width=True)

            # Bi·ªÉu ƒë·ªì c·∫•u tr√∫c m·∫°ng
            st.subheader("6. C·∫•u Tr√∫c M·∫°ng Neural")
            fig_structure = plot_network_structure(n_hidden_layers, neurons_per_layer)
            st.plotly_chart(fig_structure, use_container_width=True)

            # Log MLflow
            mlflow.log_params({
                "n_hidden_layers": n_hidden_layers,
                "neurons_per_layer": neurons_per_layer,
                "epochs": epochs,
                "batch_size": batch_size,
                "learning_rate": st.session_state['learning_rate'],
                "activation": activation,
                "samples": max_samples,
                "train_size": len(X_train),
                "test_size": len(X_test),
                "log_time": f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            })
            mlflow.log_metrics({
                "train_accuracy": float(train_acc),
                "test_accuracy": float(test_acc),
                "train_loss": float(train_loss),
                "test_loss": float(test_loss)
            })
            mlflow.keras.log_model(model, "model")

            # Hi·ªÉn th·ªã th√¥ng tin MLflow
            st.subheader("7. Th√¥ng Tin ƒê∆∞·ª£c Ghi L·∫°i")
            runs = mlflow.search_runs()
            expected_columns = ['params.n_hidden_layers', 'params.neurons_per_layer', 'params.epochs',
                                'params.batch_size', 'params.learning_rate', 'params.activation',
                                'params.samples', 'metrics.train_accuracy', 'metrics.test_accuracy',
                                'metrics.train_loss', 'metrics.test_loss']
            for col in expected_columns:
                if col not in runs.columns:
                    runs[col] = None
            st.dataframe(runs[['run_id', 'params.log_time'] + expected_columns])

if __name__ == "__main__":
    from tensorflow.keras.datasets import mnist
    (X_full, y_full), (_, _) = mnist.load_data()
    train_mnist(X_full, y_full)