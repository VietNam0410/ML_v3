import os
import streamlit as st
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
import mlflow
import mlflow.keras
import plotly.graph_objects as go
from sklearn.model_selection import train_test_split
from datetime import datetime

# Thi·∫øt l·∫≠p MLflow
DAGSHUB_MLFLOW_URI = "https://dagshub.com/VietNam0410/ML_v3.mlflow"
mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
os.environ["MLFLOW_TRACKING_USERNAME"] = "VietNam0410"
os.environ["MLFLOW_TRACKING_PASSWORD"] = "c9db6bdcca1dfed76d2af2cdb15a9277e6732d6b"

# Callback ƒë·ªÉ c·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh
class ProgressCallback(Callback):
    def __init__(self, total_epochs):
        super().__init__()
        self.total_epochs = total_epochs
        self.progress_bar = st.progress(0)
        self.status_text = st.empty()

    def on_epoch_end(self, epoch, logs=None):
        progress = (epoch + 1) / self.total_epochs
        self.progress_bar.progress(progress)
        self.status_text.text(f"Epoch {epoch + 1}/{self.total_epochs} - Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f}")

# H√†m x·ª≠ l√Ω ·∫£nh (ƒë·ªìng b·ªô v·ªõi demo)
def preprocess_image(image):
    image = image.astype('float32') / 255.0
    image = image.reshape(-1, 28, 28)
    return image

def plot_network_structure(n_hidden_layers, neurons_per_layer):
    layers = ['Input (784)'] + [f'Hidden {i+1} ({neurons_per_layer})' for i in range(n_hidden_layers)] + ['Output (10)']
    x = list(range(len(layers)))
    y = [784] + [neurons_per_layer] * n_hidden_layers + [10]
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=x, y=y, mode='lines+markers', name='S·ªë n∆°-ron',
                             line=dict(color='blue'), marker=dict(size=10)))
    fig.update_layout(
        title="C·∫•u Tr√∫c Neural Network",
        xaxis_title="L·ªõp",
        yaxis_title="S·ªë N∆°-ron",
        xaxis=dict(tickmode='array', tickvals=x, ticktext=layers),
        height=400
    )
    return fig

def train_mnist(X_full, y_full):
    st.title("üß† Hu·∫•n Luy·ªán Neural Network")

    if 'learning_rate' not in st.session_state:
        st.session_state['learning_rate'] = 0.001

    # Ch·ªçn d·ªØ li·ªáu
    st.subheader("1. Ch·ªçn D·ªØ Li·ªáu")
    total_samples = X_full.shape[0]
    max_samples = st.number_input("S·ªë m·∫´u hu·∫•n luy·ªán", min_value=1000, max_value=total_samples, value=10000, step=100)
    if max_samples < total_samples:
        indices = np.random.choice(total_samples, max_samples, replace=False)
        X, y = X_full[indices], y_full[indices]
    else:
        X, y = X_full, y_full
    X = preprocess_image(X)
    y_cat = to_categorical(y, 10)
    st.write(f"ƒê√£ ch·ªçn {max_samples} m·∫´u ƒë·ªÉ hu·∫•n luy·ªán.")

    # Chia d·ªØ li·ªáu
    st.subheader("2. Chia D·ªØ Li·ªáu")
    test_size = st.number_input("T·ª∑ l·ªá t·∫≠p test (%)", min_value=10, max_value=50, value=20, step=1) / 100
    val_size = st.number_input("T·ª∑ l·ªá t·∫≠p validation (%)", min_value=10, max_value=50, value=20, step=1) / 100
    train_size = 1 - test_size - val_size
    if train_size <= 0:
        st.error("T·ªïng t·ª∑ l·ªá train, validation v√† test ph·∫£i nh·ªè h∆°n 100%!")
        return

    X_temp, X_test, y_temp, y_test = train_test_split(X, y_cat, test_size=test_size, random_state=42, stratify=y)
    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=val_size/(1-test_size), random_state=42)

    st.write(f"T·ª∑ l·ªá: Train {train_size*100:.1f}%, Validation {val_size*100:.1f}%, Test {test_size*100:.1f}%")
    st.write(f"S·ªë m·∫´u: Train {len(X_train)}, Validation {len(X_val)}, Test {len(X_test)}")

    # Tham s·ªë hu·∫•n luy·ªán
    st.subheader("3. Thi·∫øt L·∫≠p Tham S·ªë")
    col1, col2 = st.columns(2)
    with col1:
        n_hidden_layers = st.number_input("S·ªë l·ªõp ·∫©n", min_value=1, max_value=10, value=2)
        neurons_per_layer = st.number_input("S·ªë n∆°-ron m·ªói l·ªõp", min_value=16, max_value=1024, value=128, step=16)
        epochs = st.number_input("S·ªë v√≤ng l·∫∑p (epochs)", min_value=1, max_value=100, value=5)
    with col2:
        batch_size = st.number_input("K√≠ch th∆∞·ªõc batch", min_value=16, max_value=512, value=64, step=16)
        learning_rate = st.number_input("T·ªëc ƒë·ªô h·ªçc (Œ∑)", min_value=0.00001, max_value=0.1, value=float(st.session_state['learning_rate']), step=0.00001, key="learning_rate_input")
        activation = st.selectbox("H√†m k√≠ch ho·∫°t", ['relu', 'sigmoid', 'tanh'], index=0)
        dropout_rate = st.number_input("T·ª∑ l·ªá Dropout", min_value=0.0, max_value=0.5, value=0.2, step=0.05)

    # T√πy ch·ªânh t√™n run
    run_name = st.text_input("T√™n Run (ƒë·ªÉ tr·ªëng ƒë·ªÉ t·ª± ƒë·ªông t·∫°o)", value="")
    if not run_name:
        run_name = f"Train_{n_hidden_layers}_{neurons_per_layer}_{epochs}"

    if learning_rate != st.session_state['learning_rate']:
        st.session_state['learning_rate'] = learning_rate
        st.write(f"ƒê√£ c·∫≠p nh·∫≠t t·ªëc ƒë·ªô h·ªçc: **{learning_rate}**")

    # Hu·∫•n luy·ªán
    if st.button("B·∫Øt ƒê·∫ßu Hu·∫•n Luy·ªán"):
        # X√¢y d·ª±ng m√¥ h√¨nh
        model = Sequential()
        model.add(Flatten(input_shape=(28, 28)))
        for _ in range(n_hidden_layers):
            model.add(Dense(neurons_per_layer, activation=activation))
            model.add(Dropout(dropout_rate))
        model.add(Dense(10, activation='softmax'))

        # Bi√™n d·ªãch m√¥ h√¨nh
        optimizer = Adam(learning_rate=st.session_state['learning_rate'])
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

        # Hu·∫•n luy·ªán m√¥ h√¨nh
        mlflow.set_experiment("MNIST_Neural_Network")
        with mlflow.start_run(run_name=run_name) as run:
            run_id = run.info.run_id  # L·∫•y run_id t·ª´ MLflow run hi·ªán t·∫°i
            progress_callback = ProgressCallback(epochs)
            history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, 
                               validation_data=(X_val, y_val), callbacks=[progress_callback], verbose=0)

            # ƒê√°nh gi√° tr√™n c√°c t·∫≠p
            train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
            val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
            test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)

            # Ki·ªÉm tra overfitting
            if train_acc - val_acc > 0.1:
                st.warning("**C·∫£nh b√°o**: ƒê·ªô ch√≠nh x√°c tr√™n t·∫≠p train cao h∆°n ƒë√°ng k·ªÉ so v·ªõi t·∫≠p validation. M√¥ h√¨nh c√≥ th·ªÉ ƒëang overfitting.")

            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.subheader("4. K·∫øt Qu·∫£ Hu·∫•n Luy·ªán")
            st.success(f"**ƒê·ªô ch√≠nh x√°c (Train)**: {train_acc:.4f}")
            st.success(f"**ƒê·ªô ch√≠nh x√°c (Validation)**: {val_acc:.4f}")
            st.success(f"**ƒê·ªô ch√≠nh x√°c (Test)**: {test_acc:.4f}")
            st.success(f"**M·∫•t m√°t (Train)**: {train_loss:.4f}")
            st.success(f"**M·∫•t m√°t (Validation)**: {val_loss:.4f}")
            st.success(f"**M·∫•t m√°t (Test)**: {test_loss:.4f}")

            # Bi·ªÉu ƒë·ªì l·ªãch s·ª≠ hu·∫•n luy·ªán
            st.subheader("5. Bi·ªÉu ƒê·ªì Hi·ªáu Su·∫•t Theo Epoch")
            fig = go.Figure()
            fig.add_trace(go.Scatter(x=list(range(1, epochs+1)), y=history.history['accuracy'], mode='lines+markers', name='Train Accuracy'))
            fig.add_trace(go.Scatter(x=list(range(1, epochs+1)), y=history.history['val_accuracy'], mode='lines+markers', name='Validation Accuracy'))
            fig.add_trace(go.Scatter(x=list(range(1, epochs+1)), y=history.history['loss'], mode='lines+markers', name='Train Loss'))
            fig.add_trace(go.Scatter(x=list(range(1, epochs+1)), y=history.history['val_loss'], mode='lines+markers', name='Validation Loss'))
            fig.update_layout(title="ƒê·ªô ch√≠nh x√°c v√† M·∫•t m√°t qua c√°c Epoch", xaxis_title="Epoch", yaxis_title="Gi√° tr·ªã", height=500)

            # Ki·ªÉm tra v√† log bi·ªÉu ƒë·ªì n·∫øu kaleido c√≥ s·∫µn
            try:
                fig.write_image("training_history.png")
                mlflow.log_artifact("training_history.png")
                st.success("Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c xu·∫•t v√† log th√†nh c√¥ng.")
            except ValueError as e:
                st.warning(f"Kh√¥ng th·ªÉ xu·∫•t bi·ªÉu ƒë·ªì th√†nh ·∫£nh do thi·∫øu kaleido. Vui l√≤ng c√†i ƒë·∫∑t: `pip install -U kaleido`. Bi·ªÉu ƒë·ªì v·∫´n ƒë∆∞·ª£c hi·ªÉn th·ªã.")
            st.plotly_chart(fig, use_container_width=True)

            # Bi·ªÉu ƒë·ªì so s√°nh
            st.subheader("6. So S√°nh Train, Validation v√† Test")
            fig_compare = go.Figure()
            fig_compare.add_trace(go.Bar(x=['Train', 'Validation', 'Test'], y=[train_acc, val_acc, test_acc], name='Accuracy', marker_color=['blue', 'orange', 'purple']))
            fig_compare.add_trace(go.Bar(x=['Train', 'Validation', 'Test'], y=[train_loss, val_loss, test_loss], name='Loss', marker_color=['green', 'red', 'pink']))
            fig_compare.update_layout(title="So s√°nh ƒê·ªô ch√≠nh x√°c v√† M·∫•t m√°t", xaxis_title="T·∫≠p d·ªØ li·ªáu", yaxis_title="Gi√° tr·ªã", barmode='group', height=400)
            st.plotly_chart(fig_compare, use_container_width=True)

            # Bi·ªÉu ƒë·ªì c·∫•u tr√∫c m·∫°ng
            st.subheader("7. C·∫•u Tr√∫c M·∫°ng Neural")
            fig_structure = plot_network_structure(n_hidden_layers, neurons_per_layer)
            st.plotly_chart(fig_structure, use_container_width=True)

            # Log MLflow
            mlflow.log_params({
                "n_hidden_layers": n_hidden_layers,
                "neurons_per_layer": neurons_per_layer,
                "epochs": epochs,
                "batch_size": batch_size,
                "learning_rate": st.session_state['learning_rate'],
                "activation": activation,
                "dropout_rate": dropout_rate,
                "samples": max_samples,
                "train_size": len(X_train),
                "val_size": len(X_val),
                "test_size": len(X_test),
                "log_time": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
            mlflow.log_metrics({
                "train_accuracy": float(train_acc),
                "val_accuracy": float(val_acc),
                "test_accuracy": float(test_acc),
                "train_loss": float(train_loss),
                "val_loss": float(val_loss),
                "test_loss": float(test_loss)
            })
            # Log model t·∫°m th·ªùi
            mlflow.keras.log_model(model, "model")

            # ƒêƒÉng k√Ω m√¥ h√¨nh n·∫øu hi·ªáu su·∫•t t·ªët
            if test_acc > 0.9:
                mlflow.keras.log_model(model, "model", registered_model_name="mnist_model_best")
                st.success("M√¥ h√¨nh ƒë·∫°t hi·ªáu su·∫•t t·ªët (test_accuracy > 0.9), ƒë√£ ƒëƒÉng k√Ω v√†o Registered Models v·ªõi t√™n 'mnist_model_best'.")

            # Hi·ªÉn th·ªã th√¥ng tin MLflow
            st.subheader("8. Th√¥ng Tin ƒê∆∞·ª£c Ghi L·∫°i")
            runs = mlflow.search_runs()
            expected_columns = ['params.n_hidden_layers', 'params.neurons_per_layer', 'params.epochs',
                               'params.batch_size', 'params.learning_rate', 'params.activation',
                               'params.dropout_rate', 'params.samples', 
                               'metrics.train_accuracy', 'metrics.val_accuracy', 'metrics.test_accuracy', 
                               'metrics.train_loss', 'metrics.val_loss', 'metrics.test_loss', 'params.log_time']
            for col in expected_columns:
                if col not in runs.columns:
                    runs[col] = None
            st.dataframe(runs[['run_id'] + expected_columns])

if __name__ == "__main__":
    from tensorflow.keras.datasets import mnist
    (X_full, y_full), (_, _) = mnist.load_data()
    train_mnist(X_full, y_full)