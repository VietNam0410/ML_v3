import streamlit as st
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam
import mlflow
import mlflow.keras
import plotly.graph_objects as go
import os
from datetime import datetime  # Import datetime ƒë·ªÉ s·ª≠a l·ªói

# Thi·∫øt l·∫≠p MLflow
DAGSHUB_MLFLOW_URI = "https://dagshub.com/VietNam0410/ML_v3.mlflow"
mlflow.set_tracking_uri(DAGSHUB_MLFLOW_URI)
os.environ["MLFLOW_TRACKING_USERNAME"] = "VietNam0410"
os.environ["MLFLOW_TRACKING_PASSWORD"] = "c9db6bdcca1dfed76d2af2cdb15a9277e6732d6b"

def plot_network_structure(n_hidden_layers, neurons_per_layer):
    """V·∫Ω bi·ªÉu ƒë·ªì 2D m√¥ t·∫£ c·∫•u tr√∫c m·∫°ng"""
    layers = ['Input (784)'] + [f'Hidden {i+1} ({neurons_per_layer})' for i in range(n_hidden_layers)] + ['Output (10)']
    x = list(range(len(layers)))
    y = [784] + [neurons_per_layer] * n_hidden_layers + [10]
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=x, y=y, mode='lines+markers', name='S·ªë n∆°-ron',
                             line=dict(color='blue'), marker=dict(size=10)))
    fig.update_layout(
        title="C·∫•u Tr√∫c Neural Network",
        xaxis_title="L·ªõp",
        yaxis_title="S·ªë N∆°-ron",
        xaxis=dict(tickmode='array', tickvals=x, ticktext=layers),
        height=400
    )
    return fig

def train_mnist(X_full, y_full):
    st.title("üß† Hu·∫•n Luy·ªán Neural Network: Nh·∫≠n Di·ªán S·ªë MNIST")
    st.markdown("""
    D√πng Neural Network ƒë·ªÉ ƒëo√°n s·ªë vi·∫øt tay t·ª´ MNIST.  
    Ch·ªçn c√°c tham s·ªë d∆∞·ªõi ƒë√¢y v√† xem m√¥ h√¨nh h·ªçc nh∆∞ th·∫ø n√†o!  
    **L∆∞u √Ω**: K·∫øt qu·∫£ d·ª±a tr√™n t·∫≠p train (80%) v√† validation (20%) t·ª´ d·ªØ li·ªáu ƒë·∫ßu v√†o.
    """)

    # Kh·ªüi t·∫°o session_state ƒë·ªÉ l∆∞u gi√° tr·ªã learning_rate
    if 'learning_rate' not in st.session_state:
        st.session_state['learning_rate'] = 0.001  # Gi√° tr·ªã m·∫∑c ƒë·ªãnh

    # Ch·ªçn d·ªØ li·ªáu
    st.subheader("1. Ch·ªçn D·ªØ Li·ªáu")
    total_samples = X_full.shape[0]
    max_samples = st.slider("S·ªë m·∫´u hu·∫•n luy·ªán", 1000, total_samples, 10000, step=1000)
    if max_samples < total_samples:
        indices = np.random.choice(total_samples, max_samples, replace=False)
        X, y = X_full[indices], y_full[indices]
    else:
        X, y = X_full, y_full
    X = X.reshape(-1, 28, 28).astype('float32') / 255.0  # Chu·∫©n h√≥a
    y_cat = to_categorical(y, 10)
    st.write(f"ƒê√£ ch·ªçn {max_samples} m·∫´u ƒë·ªÉ hu·∫•n luy·ªán.")

    # Tham s·ªë hu·∫•n luy·ªán (m·∫∑c ƒë·ªãnh t·ªëi ∆∞u)
    st.subheader("2. Thi·∫øt L·∫≠p Tham S·ªë")
    st.markdown("T√πy ch·ªânh m·∫°ng c·ªßa b·∫°n (c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ train nhanh):")
    col1, col2 = st.columns(2)
    with col1:
        n_hidden_layers = st.slider("S·ªë l·ªõp ·∫©n", 1, 3, 2, help="S·ªë l·ªõp x·ª≠ l√Ω d·ªØ li·ªáu (2 l√† t·ªëi ∆∞u).")
        neurons_per_layer = st.selectbox("S·ªë n∆°-ron m·ªói l·ªõp", [64, 128, 256], index=1, help="128 n∆°-ron l√† c√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô v√† hi·ªáu su·∫•t.")
        epochs = st.slider("S·ªë v√≤ng l·∫∑p (epochs)", 5, 20, 5, help="5 epochs l√† ƒë·ªß ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c t·ªët.")
    with col2:
        batch_size = st.selectbox("K√≠ch th∆∞·ªõc batch", [32, 64, 128], index=1, help="64 l√† t·ªëi ∆∞u cho t·ªëc ƒë·ªô.")
        learning_rate = st.number_input(
            "T·ªëc ƒë·ªô h·ªçc (Œ∑)", 
            min_value=0.00001,  # Gi·∫£m min_value ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
            max_value=0.1, 
            value=float(st.session_state['learning_rate']), 
            step=0.00001,  # Gi·∫£m step ƒë·ªÉ nh·∫≠p chu·∫©n x√°c h∆°n
            key="learning_rate_input",
            help="Nh·∫≠p t·ªëc ƒë·ªô h·ªçc (g·ª£i √Ω: 0.001 l√† ph√π h·ª£p nh·∫•t cho MNIST, th·ª≠ t·ª´ 0.0001 ƒë·∫øn 0.01 v·ªõi ƒë·ªô ch√≠nh x√°c cao)."
        )
        activation = st.selectbox("H√†m k√≠ch ho·∫°t", ['relu', 'sigmoid', 'tanh', 'softmax'], index=0, 
                                  help="ReLU th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët nh·∫•t.")

    # C·∫≠p nh·∫≠t session_state v·ªõi gi√° tr·ªã learning_rate m·ªõi
    if learning_rate != st.session_state['learning_rate']:
        st.session_state['learning_rate'] = learning_rate
        st.write(f"ƒê√£ c·∫≠p nh·∫≠t t·ªëc ƒë·ªô h·ªçc: **{learning_rate}**")

    # Hu·∫•n luy·ªán
    if st.button("B·∫Øt ƒê·∫ßu Hu·∫•n Luy·ªán"):
        # X√¢y d·ª±ng m√¥ h√¨nh
        model = Sequential()
        model.add(Flatten(input_shape=(28, 28)))
        for _ in range(n_hidden_layers):
            model.add(Dense(neurons_per_layer, activation=activation if activation != 'softmax' else 'relu'))
            model.add(Dropout(0.2))  # Dropout c·ªë ƒë·ªãnh
        model.add(Dense(10, activation='softmax'))

        # Bi√™n d·ªãch m√¥ h√¨nh
        optimizer = Adam(learning_rate=st.session_state['learning_rate'])
        model.compile(optimizer=optimizer, 
                      loss='categorical_crossentropy', 
                      metrics=['accuracy'])

        # MLflow
        mlflow.set_experiment("MNIST_Neural_Network")
        with mlflow.start_run(run_name=f"Train_{n_hidden_layers}_{neurons_per_layer}_{epochs}"):
            # Hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh 100% ngay l·∫≠p t·ª©c
            progress_bar = st.progress(1.0)
            st.write("Hu·∫•n luy·ªán ho√†n t·∫•t (thanh ti·∫øn tr√¨nh gi·∫£ l·∫≠p 100%).")

            # Hu·∫•n luy·ªán m√¥ h√¨nh (·∫©n qu√° tr√¨nh, ch·ªâ l·∫•y k·∫øt qu·∫£)
            history = model.fit(X, y_cat, epochs=epochs, batch_size=batch_size, 
                              validation_split=0.2, verbose=0)

            # Hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß k·∫øt qu·∫£
            st.subheader("3. K·∫øt Qu·∫£ Hu·∫•n Luy·ªán")
            train_loss = min(history.history['loss'])
            val_loss = min(history.history['val_loss'])
            train_acc = max(history.history['accuracy'])
            val_acc = max(history.history['val_accuracy'])
            
            st.success(f"**ƒê·ªô ch√≠nh x√°c (Train): {train_acc:.4f}**")
            st.success(f"**ƒê·ªô ch√≠nh x√°c (Validation): {val_acc:.4f}**")
            st.success(f"**M·∫•t m√°t (Train): {train_loss:.4f}**")
            st.success(f"**M·∫•t m√°t (Validation): {val_loss:.4f}**")

            # Bi·ªÉu ƒë·ªì ƒë·ªô ch√≠nh x√°c v√† m·∫•t m√°t qua c√°c epoch
            st.subheader("4. Hi·ªáu Su·∫•t Qua C√°c Epoch")
            fig = go.Figure()
            fig.add_trace(go.Scatter(y=history.history['accuracy'], mode='lines', name='Train Accuracy', line=dict(color='blue')))
            fig.add_trace(go.Scatter(y=history.history['val_accuracy'], mode='lines', name='Validation Accuracy', line=dict(color='orange')))
            fig.add_trace(go.Scatter(y=history.history['loss'], mode='lines', name='Train Loss', line=dict(color='green')))
            fig.add_trace(go.Scatter(y=history.history['val_loss'], mode='lines', name='Validation Loss', line=dict(color='red')))
            fig.update_layout(title="Hi·ªáu su·∫•t qua c√°c epoch", xaxis_title="Epoch", yaxis_title="Gi√° tr·ªã", 
                              height=400)
            st.plotly_chart(fig, use_container_width=True)

            # Bi·ªÉu ƒë·ªì c·∫•u tr√∫c m·∫°ng
            st.subheader("5. C·∫•u Tr√∫c M·∫°ng Neural")
            fig_structure = plot_network_structure(n_hidden_layers, neurons_per_layer)
            st.plotly_chart(fig_structure, use_container_width=True)

            # Log MLflow
            mlflow.log_params({
                "n_hidden_layers": n_hidden_layers,
                "neurons_per_layer": neurons_per_layer,
                "epochs": epochs,
                "batch_size": batch_size,
                "learning_rate": st.session_state['learning_rate'],
                "activation": activation,
                "samples": max_samples,
                "log_time": f"Th·ªùi gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
            })
            mlflow.log_metrics({
                "train_accuracy": float(train_acc),
                "val_accuracy": float(val_acc),
                "train_loss": float(train_loss),
                "val_loss": float(val_loss)
            })
            mlflow.keras.log_model(model, "model")

            # Hi·ªÉn th·ªã th√¥ng tin MLflow
            st.subheader("6. Th√¥ng Tin ƒê∆∞·ª£c Ghi L·∫°i")
            runs = mlflow.search_runs()
            expected_columns = ['params.n_hidden_layers', 'params.neurons_per_layer', 'params.epochs', 
                                'params.batch_size', 'params.learning_rate', 'params.activation', 
                                'params.samples', 'metrics.train_accuracy', 'metrics.val_accuracy',
                                'metrics.train_loss', 'metrics.val_loss']
            for col in expected_columns:
                if col not in runs.columns:
                    runs[col] = None
            st.dataframe(runs[['run_id', 'params.log_time'] + expected_columns])